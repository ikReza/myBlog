{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques (Boosting & Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's of 2 types\n",
    "- Bagging (Bootstrap Aggregator)\n",
    "    - Random Forest (uses multiple `decision trees`)\n",
    "- Boosting\n",
    "    - AdaBoost\n",
    "    - Gradient Boosting\n",
    "    - XGBoost\n",
    "    \n",
    "![ensemble](images/ensemble.png)\n",
    "\n",
    "![ensemble2](images/ensemble-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (bootstrap aggregation)\n",
    "\n",
    "![bagging](images/bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the traing data is D and the sub samples data are $ D_1, D_2,..., D_{t-1}, D_t $, then\n",
    "$ D_1, D_2,..., D_{t-1}, D_t < D $\n",
    "- In this way a group of models is created which will train with its corresbonding sub-sample data\n",
    "- Then if we give our test to predict, every sub-model will give independent output\n",
    "    - for `classification`, the final output will be measured by counting the majority of the vote/prediction that each sub-model provides.\n",
    "    - for `regression`,  the final output will be measured by taking `mean/median` of the prediction that each sub-model provides.\n",
    "\n",
    "![bagging2](images/bagging-2.webp)\n",
    "\n",
    "**Jargon Alert**\n",
    "- `Row sampling with replacement`: The process of taking subsample and creating a sub model in bagging\n",
    "- `Bootstrap`: The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement\n",
    "- `Aggregation`:  the aggregate is the output of Ensemble learning; \"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest is one of the most popular `sklearn` algorithm because it performs better on any problem(classification/regression) than other algorithms by default.\n",
    "\n",
    "From the above dicussion, we get sub-models by bootstrapping sub-samples. In `Random Forest` these sub-models are `decision trees`\n",
    "- It uses both `RS & FS`(Row Sampling & Feature Sampling). So, if the dataset has $D$ data with $m$ columns and $n$ rows and a sub-model have $d'$ data with $m'$ columns and $n'$ rows\n",
    "    - $d' < D$\n",
    "    - $m' < m$\n",
    "    - $n' < n$\n",
    "- Some advantages:\n",
    "    - It creates as many trees on the subset of the data and combines the output of all the trees. In this way it reduces overfitting problem in decision trees and also reduces the `variance` and therefore improves the accuracy.\n",
    "    - can automatically handle missing values\n",
    "    - No feature scaling (standardization and normalization) required in case of Random Forest \n",
    "    -  Random Forest algorithm is very stable. Even if a new data point is introduced in the dataset, the overall algorithm is not affected much since the new data may impact one tree, but it is very hard for it to impact all the trees.\n",
    "    \n",
    "It has two techniques.\n",
    "\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The differences:**\n",
    "- criterion: `gini/entropy`(classification), `mse/mae`(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gini vs entropy**\n",
    "\n",
    "- These are the functions to measure the quality of a split\n",
    "- `gini` for Gini Impurity\n",
    "- `entropy` for Information Gain\n",
    "\n",
    "$ P_{+} = $ the probability of yes\n",
    "\n",
    "$ P_{-} = $ the probability of no\n",
    "\n",
    "`gini formula:` $ 1 - (P_{+})^2 - (P_{-})^2 $\n",
    "\n",
    "`entropy formula:` $ -P_{+}*log_{2}(P_{+}) - P_{-}*log_{2}(P_{-}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "- https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why `gini impurity` preferred than `entropy`?\n",
    "\n",
    "- `gini impurity` takes shorter period of time for execution\n",
    "- `entropy` has logarithmic calculation, which takes more time for computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "##### How the root node of decision tree is selected?\n",
    "\n",
    "Ans: Based on information gain\n",
    "\n",
    "$IG = H(S) - \\frac{|A|}{|S|}H(A) - \\frac{|B|}{|S|}H(B)$\n",
    "\n",
    "where,\n",
    "\n",
    "$IG =$ Information Gain\n",
    "\n",
    "$H(S), H(A), H(B) =$ impurity of whole samples, sample A and sample B respectively where, $A+B=S$\n",
    "\n",
    "$|S|, |A|, |B| =$ length of whole samples, sample A and sample B respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if our dataset(titanic) have total `887` samples in which `342` passengers survived and other `545` passengers died, gini impuruty will be:\n",
    "\n",
    "$Giny = 2*p*(1-p)$\n",
    "\n",
    "$Gini = 2*\\frac{342}{887}*\\frac{545}{887}$\n",
    "\n",
    "$Gini = 0.4738$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gini formula:\n",
    "\n",
    "$1 - p^2 - (1-p)^2$\n",
    "\n",
    "$(1+p)(1-p)-(1-p)^2$\n",
    "\n",
    "$(1-p)(1+p)-(1-p))$\n",
    "\n",
    "$(1-p)(1+p-1+p)$\n",
    "\n",
    "$2p(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we split our data based on $Age <= 30$, we'll have \n",
    "- 525 passengers on the left side ($survived=197$, $died=328$)\n",
    "- other 362 passengers on the right side($survived=145$, $died=217$)\n",
    "\n",
    "1. Giny for left side:\n",
    "\n",
    "$Gini = 2*\\frac{197}{525}*\\frac{328}{525}$\n",
    "\n",
    "$Gini = 0.4689$\n",
    "\n",
    "2. Giny for right side:\n",
    "\n",
    "$Gini = 2*\\frac{145}{362}*\\frac{217}{362}$\n",
    "\n",
    "$Gini = 0.4802$\n",
    "\n",
    "So the information gain,\n",
    "\n",
    "$IG = H(S) - \\frac{|A|}{|S|}H(A) - \\frac{|B|}{|S|}H(B)$\n",
    "\n",
    "$IG = 0.4738 - \\frac{525}{887}*0.4689 - \\frac{362}{887}*0.4802$\n",
    "\n",
    "$IG = 0.003$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we split our data based on $Sex$, we'll have \n",
    "- 314 passengers on the left side ($survived=233$, $died=81$)\n",
    "- other 573 passengers on the right side($survived=109$, $died=464$)\n",
    "\n",
    "1. Giny for left side:\n",
    "\n",
    "$Gini = 2*\\frac{233}{314}*\\frac{81}{314}$\n",
    "\n",
    "$Gini = 0.3828$\n",
    "\n",
    "2. Giny for right side:\n",
    "\n",
    "$Gini = 2*\\frac{109}{573}*\\frac{464}{573}$\n",
    "\n",
    "$Gini = 0.3081$\n",
    "\n",
    "So the information gain,\n",
    "\n",
    "$IG = H(S) - \\frac{|A|}{|S|}H(A) - \\frac{|B|}{|S|}H(B)$\n",
    "\n",
    "$IG = 0.4738 - \\frac{314}{887}*0.3828 - \\frac{573}{887}*0.3081$\n",
    "\n",
    "$IG = 0.1393$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As information gain for the 2nd split is much better than the 1st split, our root node will be `Sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
