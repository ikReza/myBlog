{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques (Boosting & Bagging)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's of 2 types\n",
    "- Bagging (Bootstrap Aggregator)\n",
    "    - Random Forest (uses multiple `decision trees`)\n",
    "- Boosting\n",
    "    - AdaBoost\n",
    "    - Gradient Boosting\n",
    "    - XGBoost\n",
    "    \n",
    "![ensemble](images/ensemble.png)\n",
    "\n",
    "![ensemble2](images/ensemble-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (bootstrap aggregation)\n",
    "\n",
    "![bagging](images/bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the traing data is D and the sub samples data are $ D_1, D_2,..., D_{t-1}, D_t $, then\n",
    "$ D_1, D_2,..., D_{t-1}, D_t < D $\n",
    "- In this way a group of models is created which will train with its corresbonding sub-sample data\n",
    "- Then if we give our test to predict, every sub-model will give intependent output\n",
    "    - for `classification`, the final output will be measured by counting the majority of the vote/prediction that each sub-model provides.\n",
    "    - for `regression`,  the final output will be measured by taking `mean/median` of the prediction that each sub-model provides.\n",
    "\n",
    "![bagging2](images/bagging-2.webp)\n",
    "\n",
    "**Jargon Alert**\n",
    "- `Row sampling with replacement`: The process of taking subsample and creating a sub model in bagging\n",
    "- `Bootstrap`: The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement\n",
    "- `Aggregation`:  the aggregate is the output of Ensemble learning; \"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest is one of the most popular `sklearn` algorithm because it performs better on any problem(classification/regression) than other algorithms by default.\n",
    "\n",
    "From the above dicussion, we get sub-models by bootstrapping sub-samples. In `Random Forest` these sub-models are `decision trees`\n",
    "- It uses both `RS & FS`(Row Sampling & Feature Sampling). So, if the dataset has $D$ data with $m$ columns and $n$ rows and a sub-model have $d'$ data with $m'$ columns and $n'$ rows\n",
    "    - $d' < D$\n",
    "    - $m' < m$\n",
    "    - $n' < n$\n",
    "- Some advantages:\n",
    "    - It creates as many trees on the subset of the data and combines the output of all the trees. In this way it reduces overfitting problem in decision trees and also reduces the `variance` and therefore improves the accuracy.\n",
    "    - can automatically handle missing values\n",
    "    - No feature scaling (standardization and normalization) required in case of Random Forest \n",
    "    -  Random Forest algorithm is very stable. Even if a new data point is introduced in the dataset, the overall algorithm is not affected much since the new data may impact one tree, but it is very hard for it to impact all the trees.\n",
    "    \n",
    "It has two techniques.\n",
    "\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The differences:**\n",
    "- criterion: `gini/entropy`(classification), `mse/mae`(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gini vs entropy**\n",
    "\n",
    "- These are the functions to measure the quality of a split\n",
    "- `gini` for Gini Impurity\n",
    "- `entropy` for Information Gain\n",
    "\n",
    "$ P_{+} = $ the probability of yes\n",
    "\n",
    "$ P_{-} = $ the probability of no\n",
    "\n",
    "`gini formula:` $ 1 - (P_{+})^2 - (P_{-})^2 $\n",
    "\n",
    "`entropy formula:` $ -P_{+}*log_{2}(P_{+}) - P_{-}*log_{2}(P_{-}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "- https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why `gini impurity` preferred than `entropy`?\n",
    "\n",
    "- `gini impurity` takes shorter period of time for execution\n",
    "- `entropy` has logarithmic calculation, which takes more time for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
